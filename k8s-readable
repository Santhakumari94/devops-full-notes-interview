Kubernetes (k8s)
Kubernetes is a container cluster manager, that schedules, monitors and manages the containerized applications on a network cluster of computers

There are 4 parts in kubernetes
1. kubernetes master/control plane
2. kubernetes worknode
3. kubectl
4. etcd

#1. kubernetes masternode (or) kubernetes controlplane
kubernetes master is also called as "control plane" of the kubernetes which is an central node of the kuberneter cluster that takes care of scheduling, monitoring and managing the containerized applications on the cluster of computers. Kubernetes master is installed on a central computer/machine where it has access to all the other machines of the network.
	
There are 3 parts are there in kubernetes master
1.1 api manager
1.2 scheduler
1.3 controller manager

1.1 api manager
api manager acts an front-end of the kubernetes cluster, it is built on http protocol and exposed as httpendpoints to the clients/users of the kubernetes cluster. Through the help of api manager only people can interact with kubernetes cluster/masternode

When the client/user has sent an request to the api manager the api manager will
1. validate whether the incoming request is valid or not, if it is valid then it handovers the request to the scheduler for further operation

2. authentication/authorization: the api manager before accepting the request,it verifies whether the user sending the request is authorized to perform the operation or not, if not denies the request and returns an error response
So, from the above we can understand api manager acts as a gateway of receiving the requests into kubernetes master

How many ways we can interact with the api manager
There are 3 ways we can interact with api manager
1. kubeclt cli
2. rest endpoint
3. api (language)
	
#2. scheduler
upon receiving the request, the api manager handovers the request to scheduler asking to perform the operation.
The scheduler identifies an appropriate workernode of the cluster on whom the containerized application can be executed by talking to the kubelet process of the workernode. The scheduler will not run the containerized application on the container runtime, the job of bringing up the containerized application is takencare by the kubelet process running on the node.
	
#3 controller manager
controller manager is a daemon process that runs in background on the controlplane of the cluster and ensures always we reach to desired state of the cluster
There are 5 types of controllers are there
1. ReplicaSet = ensures the desired number of replicas are running on the cluster
2. DaemonSet  = ensures the pods are running across all the nodes of the cluster
3. DeploymentSet = takes care of pod updates
4. Service = offering network services to all the pods that are running on the kubernetes clsuter and ensures the communication across the pods and to the external networks
5. Job = We want to run an external program on each node of the cluster cluster to perform some one time activity which will be takecare by the job controller

#2. kubernetes worknode
kubernetes worknode is a computer/machine attached to the kubernetes master/control plane on which a pod is scheduled for execution. A worknode can be scheduled to run one or more pods on it based on the capacity of the workernode

#2. WorkerNode:
Workernode is an machine attached to the controlplane or masternode of the kubernetes cluster on which the pods are scheduled to be executed
Each workernode should be installed with 3 components as part of them
1. container runtime
container runtime like a docker has to be installed on the workernode to run containerized applications on the node
2. kubelet 
kubelet is a process that runs on each workernode of the cluster, it gathers the information about the pods and their statuses and communicates the information to the controlplane of the cluster. in addition we can think of an kubelet as a endprocess to whom controlplane communicates in scheduling and running the pods on the workernode
3. kubeproxy
kubeproxy enables the traffic to the external network for a pod on the cluster

#3.kubectl
kubectl is an cli tool provided by the kubernetes that enables us to communicate with kubemaster or controlplane in administering, monitoring and managing the kubernetes cluster.
Through the help of kubectl we can communiate with api manager of the kubernetes cluster and pass instructions asking to perform some operation on the cluster

The kubectl looks for a kubeconfig.yml file under $HOME/.kube/ directory in which the controlplane information is written, by reading the information about the controlplane the kubectl connects and communicates with api manger of the master.
	
#4. etcd
etcd is a key/value pair database where all the objects information that we created on kubernetes cluster will be stored as part of etcd database
--------------------------------------------------------------------------------------------
what is a pod?	
pod is the smallest unit within the kubernetes cluster where one or more containers are wrapped inside it and executed together. 
A pod may contain one or more containers, which might have a common lifecycle management and may share resources across them like
	1. network = both the containers wanted to be on the same network namespace, to communicate with each other through "localhost"
	2. storage = want to share the persistence volumes in accessing the underlying storage
	3. configuration = may share application configuration in common
keeping these containers together within pod, makes easy to manage those containers together.
	
Kubernetes is a declarative cluster container manager tool, here we dont specify what has to be done or how to run an containerized application to the kubernetes. rather we declare information about the contaierized application and the configuration with which the container has to be executed on the cluster.
The kubernetes will reads that configuration we provided and makes an best effort in running the container application across the nodes of the cluster.

The configuration information describing the details of containerized application we write is called "manifest" and is written in YAML format

CNI
CNI standards for container network interface is an specification standardized for networking aspects of interconnecting the containers. The CNI has provided libraries and plugins using which we can create our own CNI networking implementations. Kubernetes has an built-in support for CNI Networking

kubernetes has been built on top of CNI networking to ensure the applications that are build and deployed on kubernetes cluster can collaborate and communicate with each other.
There are lot of kubernetes CNI Implementations are there
1. calico
2. canal
3. flannel
4. kubenet
5. weave
6. aws vpc
7. kube-router

while installing the kubernetes cluster, before adding the working nodes we need to install/enable CNI networking on the master there after we need to add the worker nodes to the kubernetes cluster.
-------------------------------------------------------------------------------------------
How to setup an kubernetes cluster?
There are multiple technics in setting up an kubernetes cluster
1. minikube
minikube setup a kubernetes cluster with one-single node on windows/linux/mac machine that can be used for development/xperimental purpose on a personal computer.
	
2. eks
amazon aws has provided a fully managed elastic kubernetes cluster which can be used for production-usage

3. setting up kubernetes cluster using kubeadm, kubectl tools manually on a local or virtual machine environments

How to setup an kubernetes cluster on local machine using virtual machine environments?
#1 cluster configuration
1. Master Node - kubernetes master
2. Worker Nodes - 2 = kubelet, container runtime, kubeproxy
3. Workstation = kubectl/JFrog

windows 10
#1. virtualbox
#2. vagrant 
#3. download the Vagrantfile and place it in a directory like
d:\k8scluster

#4. 
goto the file directory k8scluster and run 
vagrant up

How to setup an kubernetes cluster?
#1. install virtualbox and extension pack on the windows10 machine
#2. install vagrant on windows10 machine
#3. create an Vagrantfile with 4 node multi-node vagrant file
	1. one node for kubernetes master
	2. two nodes for worker nodes
	3. one node for jfrog container registry
d:\kubecluster:/>

Vagrantfile
------------
Vagrant.configure(2) do | config |
    config.vm.define "kmaster" do | kmaster |
        kmaster.vm.box = "ubuntu/focal64"
        kmaster.vm.network "private_network", ip: "192.168.20.10"        
        kmaster.vm.hostname ="kmaster"
        kmaster.vm.provider "virtualbox" do | vb |
            vb.cpus=3
            vb.memory=3072
            vb.name="kmaster"
        end
    end
    %w{wnode1 wnode2}.each_with_index do | nodename, index |
        config.vm.define nodename do | node |
            node.vm.box = "ubuntu/focal64"
            node.vm.hostname = nodename
            node.vm.network "private_network", ip: "192.168.20.#{index+11}"
            node.vm.provider "virtualbox" do | vb |
                vb.cpus = 2
                vb.memory = 1024
                vb.name = nodename
            end
        end
    end
		config.vm.define "jcrnode" do | jcrnode |
        jcrnode.vm.box = "ubuntu/focal64"
        jcrnode.vm.network "private_network", ip: "192.168.20.13" 
				jcrnode.vm.network "forwarded_port", guest: 8082, host: 8082
        jcrnode.vm.hostname ="jcrnode.com"
        jcrnode.vm.provider "virtualbox" do | vb |
            vb.cpus=2
            vb.memory=1024
            vb.name="jcrnode"
        end
    end
end

#4. goto the d:\kubecluster:/> and run below command
vagrant up
brings up all the 4 machines
--------------------------------------------------------------------------------------------
ssh into all the machines using
vagrant ssh machinename
--------------------------------------------------------------------------------------------
#1. bridge traffic to iptables is enabled
add the below entries into the file: /etc/ufw/sysctl.conf

sudo vim /etc/ufw/sysctl.conf
net/bridge/bridge-nf-call-ip6tables = 1
net/bridge/bridge-nf-call-iptables = 1
net/bridge/bridge-nf-call-arptables = 1

#2. modify /etc/hosts file on all the nodes and make an entry of 4 notes with their ip address and node name
sudo vim /etc/hosts

192.168.20.10  kmaster
192.168.20.11  wnode1
192.168.20.12  wnode2
192.168.20.13  jcrnode.com

in addition to the above in each node we need to add nodename.local entry of that relevant node for routing kube-proxy traffic to the services of the cluster

kmaster node
192.168.20.10  kmaster kmaster.local

wnode1 node
192.168.20.11  wnode1 wnode1.local

wnode2 node
192.168.20.12  wnode2 wnode2.local
--------------------------------------------------------------------------------------------

#4. 
sudo apt -y update
sudo apt -y upgrade


#5. enable eithernet bridge tables
sudo apt install -y ebtables ethtool
--------------------------------------------------------------------------------------------
#6 Install docker on all the machines (apart from jcrnode)
	
1. install pre-requisite software for installing the docker 
sudo apt install apt-transport-https ca-certificates curl software-properties-common

2. Then add the GPG key for the official docker repository to our system.
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

3. Add the docker repository to the apt sources:
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"
4. update the package database with the newly added docker repo
sudo apt -y update

5. install docker packages
sudo apt install -y docker-ce

6.
sudo usermod -aG docker $USER

7. exit from kmaster and restart all the virtual machines
exit
vagrant reload
and again ssh onto all the machines

#4 enable external dns (all nodes including jcrnode)
sudo vim /etc/systemd/resolved.conf
DNS=8.8.8.8 = google dns
sudo service systemd-resolved restart

#5.
sudo vi /etc/docker/daemon.json

{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
sudo systemctl restart docker
sudo systemctl status docker

#remove config.toml from containerd directory 
sudo rm /etc/containerd/config.toml
sudo systemctl restart containerd

--------------------------------------------------------------------------------------------
#1 run on all the nodes except jcrnode
add gpg key to enable kubernetes repositories to be added to ubuntu for installing kubernetes.
be in root user and run the below 2 commands
sudo su - = switch to root user
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

#2. 
sudo apt -y update

#3 install kubectl, kubeadm, kubelet process  (run on all the nodes except jcrnode)
sudo apt install -y kubelet kubeadm kubectl

#### end of docker ######
--------------------------------------------------------------------------------------------
#1
Let us setup kubernetes master/controlplane only on kmaster node only by running the below command.
kubeadm init = command creates/installs kubernetes master on the node we ran.
sudo kubeadm init --apiserver-advertise-address=192.168.20.10 --pod-network-cidr=10.244.0.0/16

The above commands creates kubernetes master/control plane and generates a kube config file with the configuration information of the cluster. So that the kubectl uses the configuration in connecting to the cluster

The pod cidr is the ip address range we specified to the kubernetes master, asking to generate ip address within the CIDR range for the pods of the cluster.
	
The masternode and the workernodes of the cluster are on a different network from the pod network, so that we cannot access the pods that are running on the cluster from any of the master/worker nodes and those are completely isolated from the world.
	
but a pod can communicate with another pod since those are all part of the same network.
	
The cidr range of the pod network is /16 since most of the CNI implementations supports /16 as network range only

In one kubernetes cluster we can add 5000 workernodes and in production deployment it is recommended to have a minimum 5 worker nodes for a kubernetes cluster. Within the cluster of 5000 workernodes we can create upto 150000 pods on the cluster

In an on-premise environment (physical machine setup) we need to start with minimum 5 workernodes
but in an aws eks cluster we can configure the node pool size to start with a minimum = 2 and a maximum = 10
So EKS configures the kubernetes cluster initially with 2 worknodes and based on the demand it automatically scales up a max of 10 workernodes as specified.
	
upon running kubeadm init command it generates an file called /etc/kubernetes/admin.conf which contains the information about the kubernetes cluster

The kubectl looks for only $HOME/.kube/config file for connecting to the kubernetes cluster so we need to create .kube/config file with the information from admin.conf file

When we run kubeadm init command it generates 2 things
1. kubeadm join command with ca-has allowing us to join worker nodes with the master
2. commands for copying admin.conf file into $HOME/.kube/conf file as below.
	
The above command generates an ca-hash with kubeadm join command, copy and save the command aside. it should be executed only after applying pod network.
kubeadm join 192.168.20.10:6443 --token 5nosl5.0poqh3fwq62o2j61 \
        --discovery-token-ca-cert-hash sha256:55493de1ebb54aec3e5830432699a4f7a40651969007e9a6a9927aa72ca35861
				

#2. Setup the kubeconfig file,to let the kubelet connect to the kubernetes cluster.
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#####################
dont join worker nodes with the master by running kubeadm join command until we configure cni networking on the master node
########################
copy the kubeadm join command so that after configuring the cni networking, we can use the command for joining the workernodes to the cluster
-------------------------------------------------------------------------------------------
The kubeadm join command generated sha ca certificate and hash will be expired within 1 to 1 1/2 hour for security reason
to regenerate in adding the workernodes to the master we need to use the below command on the master
kubeadm token create --print-join-command

we can see existing tokens that are generated in kubernetes master using
kubeadm token list
------------------------------------------------------------------------------------------

#3 download flannel network yaml file
curl https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml  -o kube-flannel.yml

vim kube-flannel.yml
kubectl create -f kube-flannel.yml


sudo apt install etcd-client
sudo systemctl daemon-reload
sudo systemctl restart kubelet

************************ at the last install multicaste package on all the nodes ***********
sudo apt install -y avahi-daemon libnss-mdns


kubectl get pods -o wide --all-namespaces
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE    IP              NODE      NOMINATED NODE   READINESS GATES
kube-system   coredns-6d4b75cb6d-2w4m5          1/1     Running   0          17m    10.244.0.3      kmaster   <none>           <none>
kube-system   coredns-6d4b75cb6d-q42dr          1/1     Running   0          17m    10.244.0.2      kmaster   <none>           <none>
kube-system   etcd-kmaster                      1/1     Running   0          17m    192.168.20.10   kmaster   <none>           <none>
kube-system   kube-apiserver-kmaster            1/1     Running   0          17m    192.168.20.10   kmaster   <none>           <none>
kube-system   kube-controller-manager-kmaster   1/1     Running   0          17m    192.168.20.10   kmaster   <none>           <none>
kube-system   kube-flannel-ds-r68xb             1/1     Running   0          110s   192.168.20.10   kmaster   <none>           <none>
kube-system   kube-proxy-4v4hw                  1/1     Running   0          17m    192.168.20.10   kmaster   <none>           <none>
kube-system   kube-scheduler-kmaster            1/1     Running   0          17m    192.168.20.10   kmaster   <none>           <none>
--------------------------------------------------------------------------------------------
Now let us setup worker node
#1. change the ip address of workernode /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

sudo vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
"Environment KUBELET_EXTRA_ARGS=--node-ip=192.168.20.12"

#2. join the each worker node to master

sudo kubeadm join 192.168.20.10:6443 --token 5nosl5.0poqh3fwq62o2j61 \
        --discovery-token-ca-cert-hash sha256:55493de1ebb54aec3e5830432699a4f7a40651969007e9a6a9927aa72ca35861

sudo systemctl daemon-reload
sudo systemctl restart kubelet

************************ at the last install multicaste package on all the nodes ***************
sudo apt install -y avahi-daemon libnss-mdns
--------------------------------------------------------------------------------------------
by default the pods are not scheduled on master node, if we want the pods to run on the master node as well by treating master as workernode then we need to taint the master.
	

--------------------------------------------------------------------------------------------
#1. jfrog jcr registry
1. jfrog jcr requires domain name resolution to work
so to host jcr on a domain name we need to setup virtualhost configuration use apache2 server
	1.1 install apache2
		sudo apt update -y
		sudo apt install -y apache2
	1.2 enable apache2 modules
		sudo a2enmod proxy
		sudo a2enmod proxy_http
		sudo a2enmod proxy_loadbalancer
		sudo a2enmod lbmethod_byrequests
	1.3 virtual host configuration file
	/etc/apache2/sites-available/jcrhub.conf
	<VirtualHost *:80>
		ServerName jcrnode.com
		ProxyPreserveHost on
		ProxyPass / http://127.0.0.1:8082/
		ProxyPassReverse / http://127.0.0.1:8082/		
	</VirtualHost>
	1.4 enable the virtualhost configuration
	sudo a2ensite jcrhub
	sudo systemctl daemon-reload
	sudo systemctl restart apache2
	
#2 jfrog jcr registry setup
#1. download from: https://jfrog.com/container-registry/ (jfrog-artifactory-jcr-7.37.14-linux.tar.gz)
#2. create an directory 
mkdir -p /u01/middleware
#3. copy the tar.gz file into /u01/middleware and extract it
mv ~/Downloads/jfrog-artifactory-jcr-7.37.14-linux.tar.gz /u01/middleware
cd /u01/middlware
tar -xzvf jfrog-artifactory-jcr-7.37.14-linux.tar.gz

#4. with default configurations we can launch the jfrog artifactory jcr registry
goto artifactory-jcr-7.37.14/app/bin
cd /u01/middlware/artifactory-jcr-7.37.14/app/bin
./artifactory.sh

The artifactory jcr registry will run by default on 8082 port, that is the reason we configured apache2 virtualhost configuration pointing to http://localhost:8082/
------------------------------------------------------------------------------------------
#3 dns hostname configuration on localhost
goto C:\Windows\System32\drivers\etc and add hosts file below entry
192.168.20.13  jcrnode.com	



by default the docker uses htttps for connecting to the docker container registry, so we need to modify daemon.json to ignore https, add this configuration in all the 3 nodes


sudo vi /etc/docker/daemon.json
{
  "insecure-registries" : ["jcrnode.com"]
}
sudo systemctl restart docker


What is kubernetes?
Kubernetes is a cluster container manager, which is used for scheduling, running, monitoring and managing the containerized applications on a network of computers. we can
1. schedule and run containers on workernodes
2. we can scaleup the infrastructure by adding workernodes to the cluster
3. able to restart the failed containers
4. we can setup a network of containers so that they can communicate with each other
5. pod deployments and rolling upgrades
--------------------------------------------------------------------------------------------

Kubernetes Namespaces
---------------------
Namespaces are used for creating compartments or logical grouping of objects/resources on the kubernetes cluster.
For eg.. when we are running multiple projects on the kubernetes cluster we can ensure no 2 people can see the resources of the each of their other projects throught then the help of namespaces.
The kubernetes administrator can create users, namespaces. he assigns users to the namespaces granting access to the users to those namespaces only.
	
The user can see/manage the objects of that namespace only and cannot see or administer the objects of another namespace. in this way we are able to isolate the objects/resources of the kubernetes cluster

By default as part of the kubernetes install there are 4 namespaces created
1. default = by default any objects created in kubernetes cluster will be placed under default namespace. by default it is empty. everyone can access the default namespace and should be sufficient for most of the usecases
2. kube-system = This namespace holds the objects related to kubernetes system like api manager, scheduler and controller manager etc
3. kube-public = by default it doesnt have any objects and if we place any objects on kube-public those are accessible to anyone without authentication
4. kube-node-lease = all the kubernetes leased objects are associated to this namespace


#1. How to create our own namespace in kubernetes cluster?
kubectl create namespace namespacename

#2. How to see the namespaces
kubectl config view
The above command displays the kubeconfig file used by the kubectl, in which the namespace being used will be specified. if there is no namespace entry we are using "default" namespace
#1. how to see the kube config file of the cluster?
kubectl config view

#2. How to see the pod objects that are in specific namespace?
kubectl get pods -o wide -n namespace

#3. how to create a namespace?
kubectl create namespace namespacename
--------------------------------------------------------------------------------------------
How to manage multiple kubernetes clusters through kubectl?
Within an organization we might have multiple kubernetes clusters, for eg..
	
A cluster for development environment = where developers will deploy their containerized appliations and test it in their development environment
A cluster for testing = qa deploys the applications delivered by developers for testing

We want to connect to any of these kubernetes cluster from an workstation and manage these environments using kubectl. This can be managed by configuring kube config file

The kubectl by default reads the cluster information from $HOME/.kube/config file in connecting to the kubernetes cluster. In the .kube/config file we can specify multiple kubernetes cluster information

we can change the default file that kubectl reads for connecting to the cluster by setting up an environment variables pointing to a different file as below.
$HOME/sailor/.kube/sailorclusterconfig
export KUBECONFIG=$HOME/sailor/.kube/sailorclusterconfig

To manage connecting to the multiple different clusters we can add multiple cluster informations in kube config file as below.
	
$HOME/.kube/config
------------------
apiVersion: v1
kind: Config
preferences: {}
clusters:
- cluster:
	name: development-cluster
		server: http://ip:port
		certificate-authority-data: key
- cluster:
	name: test-cluster
		server: http://ip:port
		certificate-authority-data: key		
- cluster:
	name: development
		server: http://ip:port
		
users:
- name: kubernetes-developer-admin
	user:
		client-certificate-data:
		client-key-data:
- name: kubernetes-test-admin
	user:
		client-certificate-data:
		client-key-data:

contexts:
- context: 
		cluster: development-cluster
		user: kubernetes-developer-admin
		namespace: x
	name: development-context
- context:
		cluster: test-cluster
		user: kubernetes-test-admin
		namespace: testnamespace
	name: test-context
current-context: development-context


To switch from one cluster to another cluster we need to change current-context in kube config file.
	
There are 2 ways we can change kube config file
1. we can modify the kubeconfig file manually through text editor
2. through kubectl commands

1#How to change the namespace of the an kubernetes cluster we want to access
kubectl config set-context --current --namespace=x

2#How to see the current context we are using?
kubectl config current-context

3#How to see all the contexts
kubectl config get-contexts

#4 How to switch the context
kubectl config use-context contextname


How to add a new kubernetes cluster information in kubeconfig file?
There are 2 ways we can do this
1. modify the kubeconfig file manually and add the cluster information
2. we can add the new cluster using --set-cluster command of kubectl

kubectl config --kubeconfig=config set-cluster development --server https://ip:port --insecure-skip-tls-verify

Kubernetes Objects
-------------------
There are different types of resources are there in kubernetes cluster
1. Pod
2. Deployments
3. ReplicaSet
4. Service
5. Job
6. DaemonSet
etc

The kubernetes will represent the information about all the resources interms of Objects and persist them in etcd of the cluster. All these objects are created under kube-system namespace
The kubernetes objects represents the state of the kubernetes cluster

What does a kubernetes object represents?
1. It represents the contaierzied application that is scheduled/running on the cluster
2. Resources allocated in running the application
3. The policy bindings to the resource

A kubernetes objects is a record of intent, once we create an kubernetes object, the system constantly work to ensure the object exists/created on the cluster. Through the object we are specifying to the kubernetes system, what we want on the cluster

These kubernetes objects can be created by writing resource spec or manifest file, and should be passed as an input to the controlplane using kubectl.
The control plane reads the resourcespec/manifest and creates an appropriate kubernetes object and stores/persist in etcd

resourcespec -> kubectl-> controlplane -> object -> stores(etcd)
	
The resource spec or manifest is an YAML file that has to be written describing the object we wanted to be created on the cluster as key/value pair and pass it as an intput to control plane
Required Field:-
	apiVersion: = which version of the kubernetes object we are using
	kind = type of object
	metadata = used to define labels
	namespace = the object should be created under which namespace
	spec = desired state of the objects
-------------------------------------------------------------------------------------------
How to create and manage these objects in kubernetes cluster?
The kubectl has provided sophisticated ways in creating and managing these objects on kubernetes cluster. There are 3 ways we can create these objects
1. imperative commands
2. imperative object configuration
3. declarative object configuration

How to create and manage the kubernetes objects on the cluster?
The kubectl command supports 3 ways of creating the objects on the cluster
1. Imperative commands
kubectl has provided handful commands to which we can pass input arguments in creating various different types of kubernetes objects on the cluster. we dont need to manually write resource spec/manifest in creating the objects on the cluster

kubectl run apache2 --image=apache2 --port=80

advantages:-
1. quickly can create objects on the cluster without writing any manifest

dis-advantages:-
1. we dont have a spec in hand that can be tweaked or modified or reused at later point of time


2. Imperative object configuration


3. declarative object configurtion

by default the docker uses htttps for connecting to the docker container registry, so we need to modify daemon.json to ignore https, add this configuration in all the 3 nodes


sudo vi /etc/docker/daemon.json
{
  "insecure-registries" : ["jcrnode.com"]
}
sudo systemctl restart docker
-------------------------------------------------------------------------------------------
How many ways we can create kubernetes objects on the kubernetes cluster?
The kubectl provides 3 ways in creating the kubernetes objects on the cluster
1. imperative commands
The kubectl has provided handful of commands taking arguments as input in creating various different kubernetes objects like pods, service, deployments etc. we dont need to write resource spec or manifest in creating the objects on the cluster

kubectl run podName --image=imageName --port=portNo
The above quickly creates an pod on the cluster with given image and port to be exposed

advantages:-
	1. quickly we can create an kubernetes object on the cluster without writing any spec
dis-advantage:-
	1. since we dont have an resource spec in creating the object, we cannot modify or create another object on a different environment

2. imperative object configuration
In an imperative object configuration, we write resource spec or manifest describing the information about the object we want to create on the cluster in an ".yml" file and pass the manifest to kubectl asking to perform operations like create, delete etc

kubectl create -f apache-pod.yml
kubectl delete -f apache-pod.yml

3. declarative object configuration
There can be group of resources that has to be created on the k8s cluster to achieve the desired state of the system. For each of these resources describing their information we have created manifests
These manifests has to be applied on the cluster in specific order based on the dependencies of these resources. 
applying individual manifest by deriving the dependency order of resources takes lot of time, instead use declarative object configuration commands

keep all the resource specifications/manifests within a directory
configs/
	pods.yml
	deployments.yml
	service.yml	
pass the directory as an input to the kubectl tool asking to apply these manifests on the cluster
kubectl apply -f configs/
kubectl delete -f configs/
	
How to write a pod spec and create the pod object on the cluster?
pod is an smallest entity on the k8s cluster where a group of containers which shares common dependencies like resources(files/mount locations)	or lifecycle are put together to be executed within an pod

ubuntu/apache2:2.4-22.04_beta

apache2-pod.yml
-----------------
apiVersion: v1
kind: Pod
metadata:
	name: apache2pod
spec:
	containers:
		- name: apache2
			image: jcrnode.com/docker/ubuntu/apache2:2.4-22.04_beta
			ports:
				- containerPort: 80
					name: http
					protocol: TCP

kubectl create -f apache2-pod.yml

How to work with running an pod on kubernetes cluster using imperative configurtion objects?
write the pod spec (yaml file) describing the information about the container application we want to run on the cluster
pod manifest:

apache2-pod.yml
----------------
apiVersion: v1
kind: Pod
metadata:
	name: apache2pod
spec:
	containers:
		- name: apache2
			image: ubuntu/apache2:2.0.28beta
			ports:
				- name: http
					containerPort: 80
					protocol: TCP
					
kubectl create -f apache2-pod.yml

#1. how to create a pod on the cluster?
kubectl create -f pod.yml -n namespace

#2. how to see the pods on the cluster?
kubectl get pods -o wide -n namespace

#3. how to see the details of a pod on the cluster?
kubectl describe pod podName

#4. how to see the logs of the running application inside the pod?
kubectl logs podName
kubectl logs -f podName = tail -f

#5. how to delete a pod
kubectl delete pod podName
kubectl delete -f pod.yml
--------------------------------------------------------------------------------------------
livelinessProbe and readinessProbe on pods

when we schedule a kubernetes pod during its execution the underlying application running inside the pod might go into unresponsive state due to various different issues like stuck thread or resources not available for serving the request and might be timeout etc.
even then also kubernetes will report the pod status as running only.
To identify such unresponsive pods and replace them with healthy pods by the kubernetes we need to help kubernetes determine the real status of the pod, which can be done through readinessProbe and livelinessProbe

Kubernetes supports 2 types of checks to be performed on the pod
1.readinessProbe
upon scheduling a pod for execution, the kubernetes after pulling the image and bringing up the pod into execution, it immediate reports the status of the pod as running and make it accessible. but in reality the underlying application running inside the pod is still starting up and may not be ready to accept the requests, even then also kubernetes assumes that it has been started and every request received from the outside world is routed to the application which will eventually fail how to solve this problem?
	
The kubernetes has introduced readinessProbe, which let the kubernetes determine when does the actually application is available for accessing. The developer of the underlying application has to expose an http endpoint which needs to be configured as part of the pod spec letting kubernetes probe the endpoint to understand when does the pod application is available for accessing to make it exposed to the world.
until the readinessProbe passed no requests will be scheduled to be recieved by the pod.

Readiness Probe and Liveliness Probe

For a pod we can configure readinessProbe and livelinessProbe letting the kubernetes identify when does the pod is available for accessing and determine whether the pod is running or not

Whenever we create an pod object on kubernetes cluster, the controlplane schedules the pod for running, upon bringing up the pod, the kubernetes control plane reports the pod is available for accessing. but the underlying applications running within the pod may not be running and are not accessible, at this time if the pod has been made accessible by the kubernetes the requests that are send by the client would fail. To let the kubernetes identify the application has been ready for usage, our application has to expose readiness endpoint 
This endpoint has to be configured as part of pod spec letting the kubernetes identify the pod is ready for accessing.
The readiness check will be done by controlplan at the time of starting up the pod, there after the readiness check will not be performed

livelinessProbe
upon bringing up the pod, during its execution the application running inside the pod may go unresponsive due to memory or stuck thread issuses or resource availability, at this moment kubernetes will not be able to identify the application in-availability and continue to run the pod. 
To help the kubernetes identify such failures of the application and terminate the pod the application should expose an healthcheck endpoint that can be periodically polled by kubernetes to monitor the state of the pod application and can be replaced if necessary

urotaxi (java application)
|- /actuator/health/readiness
|- /actuator/health/liveness

urotaxi-pod.yaml
----------------
apiVersion: v1
kind: Pod
metadata:
	name: urotaxipod
	labels:
		app: urotaxi
spec:
	containers:
		- name: urotaxi
			image: techsriman/urotax:1.0
			ports:
				- name: http
				  containerPorts: 8080
					protocol: TCP
			readinessProbe:
				httpGet:
					path: /actuator/health/readiness
					port: 8080
				initialDelaySeconds: 10
				timeoutSeconds: 10
				failureThreadshold: 3
			livenessProbe:
				httpGet:
					path: /actuator/health/liveness
					port: 8080
				initialDelaySeconds: 20
				timeoutSeconds: 10
				failureThreadshold: 3

Resource declarations
Whenever a pod is running on the kubernetes cluster, it is going to consume the cpu/memory during execution. The amount of cpu/memory it consumes depends on various factors
1. hardware capacity of the machine on which we are running the containerized application
2. load on the application
3. payloads with which we access the application

The performance testing team put the application for evaluation and derives bench mark metrics in terms of consumption of cpu/memory for our application

Whenever we created a pod asking the kubernetes controlplane schedule for execution, it determines the workernode on which the pod needs to be executed based on cpu/memory usage of the pod.
As part of the pod specification, we need to declare the cpu and memory usage the application required based on which the kubernetes control plan can determine the right workernode.
We need to specify minimum requirement of cpu/memory in pod spec based on which worked node will be choosen for execution. if the application running in the pod is crossing more than the minimum limits we defined, the kubelet process allocates additional resources automatically allowing the application to run if the resources are available on that node.
	
given the workernode doesnt have resources that is requested by the pod, then the pod will be terminated and an appropriate workernode will be lookedup for scheduling the pod execution

So we need to declare minimum resources required for running a pod in pod spec.
sailor-pod.yml
---------------
apiVersion: v1
kind: Pod
metadata:
	name: sailorpod
	labels:
		appName: sailor
		version: 1.0
spec:
	containers:
		- name: sailor
			image: techsriman/sailorrepo:2.0
			ports:
				- name: http
					containerPort: 8080
					protocol: TCP
			livenessProbe:
				httpGet:
					path: /sailor/actuator/liveness
					port: 8080
				initialDelaySeconds: 10
				timeoutSeconds: 10
				failureThreshold: 3
			readinessProbe:
				httpGet:
					path: /sailor/actuator/readiness
					port: 8080
				initialDelaySeconds: 10
				timeoutSeconds: 5
				failureThreashold: 3
			resources:
				requests:
					cpu: "500m"
					memory: "512Mi"	
				limits:
					cpu: "1000m"
					memory: "1024Mi"	

How to expose a pod?
kubectl expose pod hangoutpod --name hangoutsvc --port 8080 --type=NodePort
Now we can access the above pod using the workernode ip address:clusterPort

What are the different states in which a pod can exists in Kubernetes Cluster?
A pod in kubernetes can exists in 5 different states which is termed as the lifecycle of a pod
1. pending = pod has been accepted and it is under creation state
2. running = atleast one of the container inside the pod has been started and readiness probe on the pod has been passed, then the pod is reported under running state
3. succedded = when all the containers within the pod has been exited with exitcode as zero, then the pod status is reported as succeded
4. failed = when atlease one container within the pod has been exited with exitcode as non-zero, then the pod is reported as failed.
5. crashloopbackoff = when a pod is repeatedly failing for execution after a successive restarts, then to avoid further scheduling of the same pod for execution the kubernetes marks the pod as crashloopbackoff indicating not to schedule the pod for further execution since it is failing repeatedly
--------------------------------------------------------------------------------------------
Working with Labels and Annotations in kubernetes
Labels:
Labels are arbitary key/value pairs we can attach or assign to the kubernetes objects, these are used for identifying the objects and accessing them over the cluster.
For any kubernetes object like a 
 - pod
 - job
 - deploymentset
 - replicaset etc
we can assign any number of labels, but the key/value pair should appear only once and should be unique

We can define the labels for kubernetes objects in 2 ways
1. we can declare labels in manifest files of the objects directly
2. we can attach/bind labels at runtime during the execution

#1. how to declare labels as part of the manifest?
sailor-pod.yml
----------------
apiVersion: v1
kind: Pod
metadata:
	name: sailorpod
	labels:
		app: sailor
		version: 1.0
		author: sriman
		jdk: 11
spec:
	containers:
		- name: sailor
			image: techsriman/sailorrepo:2.0
			ports:
				- name: http
				  containerPort: 8080
					protocol: TCP
					
#1. We can display all the pods on the cluster along with their labels:
kubectl get pods --show-labels

#2. We can search/query a pod based on the label using -l switch/option
kubectl get pods -l app=sailor

#3. we can attach labels to the running pods at runtime using
kubectl label pods pod-name key=value
--------------------------------------------------------------------------------------------
Annotations
Annotations are used for attaching arbitary information which is non-identifier to an kubernetes object. These are only used as documentation helpers which can read or used through kubernetes metadata api. Unlike labels we cannot use annotations for accesing the objects of kubernetes


sailor-pod.yml
----------------
apiVersion: v1
kind: Pod
metadata:
	name: sailorpod
	labels:
		app: sailor
		version: 1.0
		jdk: 11
	annotations:
		author: sriman
		license: GPL license
		warranty: no warranty for this product usage
spec:
	containers:
		- name: sailor
			image: techsriman/sailorrepo:2.0
			ports:
				- name: http
				  containerPort: 8080
					protocol: TCP

What are labels and annotations in kubernetes?
Labels are the key/value pairs we can attach to any kubernetes object we create on the cluster.	These acts as an identifiers in querying and accessing the objects back from the kubernetes.
	
We create a Service or DaemonSet or ReplicaSet on a Pod by using Label as an Selector to identify the pods on the cluster

We can attach labels to the pod in 2 ways
1. we can define labels inside the object spec/manifest
2. we can add labels to a object during runtime

#1. define labels in a pod spec
pod.yml
-------
apiVersion: v1
kind: Pod
metadata:
	name: sailorpod
	labels:
		version: 1.0
		app: sailor
spec:
	containers:
		- name: sailor
			image: techsriman/sailorrepo:2.0
			ports:
				- name: http
					containerPort: 8080
					protocol: TCP

1. How to see the pods along with the labels attached
kubectl get pods --show-labels

2. How to query the pod of a specific label
kubectl get pods -l labelKey=labelValue

3. How to attach label to an running pod?
kubectl label pods podName key=value
--------------------------------------------------------------------------------------------
What are annotations?
annotations are arbitary key-value pair we can attach to any kubernetes object. unlike labels we cannot select/query the objects based on annotations, these doesnt acts as identifiers of the	object
Those are just meant for attaching documentation to the kubernetes object

we can define annotations as part of the object spec as below:
pod.yml
-------
apiVersion: v1
kind: Pod
metadata:
	name: sailorpod
	annotations:
		author: patrik
		license: GPL
spec:
	containers:
		- name: sailor
			image: techsriman/sailorrepo:2.0
			ports:
				- name: http
					containerPort: 8080
					protocol: TCP
--------------------------------------------------------------------------------------------
What are ConfigMaps and why do we need to use in Kubernetes?

Always push the configuration values required for your application into the environment/system rather than designing the application to read the configuration values from external location

In a java applications we can read the configuration values from environment variables or system properties, so that while running the application the devops engineer/developer can pass these values as an input by configuring them in the environment.
	
In an kubernetes environment we dont lauch the application on a workernode or environment, kubernetes master/controlplane takes care of scheduling the dockerized application on one of the workernode
So, we dont have a way to inject the configuration values into the containerized application environment while launching the application, because controlplane takes care of it

So how to manage in running an application with external configuration being passed as an input while running on kubernetes environment?
one way we can do this is by passing the configuration values as environment variables by defining them directly within the pod spec/manifest.
but it has quite a number of challenges,like each time there is a change in the configuration values we need to rewrite the pod spec which is not easily maintainable.
	
Instead of writing these configuration values in manifest/spec files place them in configmap as objects in kubernetes cluster. These configuration values placed in config map can be passed as an input to the pods/application in 3 ways
1. environment variables =  we can pass these config map key/values as environment variables by referring them in pod spec
2. command-line arguments = we can pass these configmap values as commandline arguments while lauching the application
3. through config map api = In our containerized application we write the code for reading the config map values and using in the application (not recommended) 
	
What are ConfigMap, what is the purpose of them?
ConfigMaps are used for externalizing the application configuration, so that we can pass them as an input while lauching the application in a kubernetes environment.
	
In order to use the configuration values through ConfigMap we need to create ConfigMap object with keys/values on kubernetes cluster.
	
#1. ConfigMap
sailor-configmap.yml
---------------------
apiVersion: v1
kind: ConfigMap
metadata:
	name: sailorconfig
	labels:
		environment: dev
data:
	driverClassname: com.mysql.cj.jdbc.Driver
	url: jdbc:mysql://localhost:3306/db
	username: root
	password: root
		
kubectl create -f sailor-configmap.yml

sailor-pod.yml
--------------
apiVersion: v1
kind: Pod
metadata:
	name: sailorpod
	labels:
		app: sailor
spec:
	containers:
		- name: sailor
			image: techsriman/sailorrepo:2.0
			ports:
				- name: http
				  containerPort: 8080
					protocol: TCP
			env:
				- name: db.driverClassname
					valueFrom:
						configMapKeyRef:
							name: sailorconfig
							key: driverClassname
				- name: db.url
					valueFrom
						configMapKeyRef:
							name: sailorconfig
							key: url

There are 3 way we can pass the configmap as an input to the pod
1. environment variables/command-line arguments
2. we can configmap with configuration files as volume mounts in podspec
3. we can access the configmap using api in the applications

2. How to pass the configmap with configuration files as volumes in podspec?

roadster-configmap.yml
-----------------------
apiVersion: v1
kind: ConfigMap
metadata:
	name: roadsterplacesconfigmap
data:
	places.properties |
		Hyderabad=Madhapur
		Chennai=Mahabalipuram
		Banglore=MG Road
		Delhi=Caunat Place
		Pune=Wakad

roadster-pod.yml
----------------
apiVersion: v1
kind: Pod
metadata:
	name: roadsterpod
	labels:
		app: roadster
spec:
	containers:
		- name: roadster
			image: techsriman/roadster:2.0
			ports:
				- name: http
					containerPort: 8082
					protocol: TCP
			volumeMounts:
				- name: roadstervolume
					mountPath: /config
					readonly: true
	volumes:
		- name: roadstervolume
			configMap:
				name: roadsterplacesconfigmap
				items:
					- key: places.properties
						path: "places.properties"

we defined a volume with name roadstervolume that contains configMap properties. now we are mounting this volume into the container under the path /config

ConfigSecrets
PersistenceVolume/Claims
DaemonSet
DeploymentSet
ReplicaSet
Service


Config Secrets
---------------
Kubernetes Secrets let you store and manage sensitive information like passwords, ssh keys, encryption keys that are required by our applications. In general we can store these secrets as part of pod spec or within configmap, but storing these secrets in pod spec or configmap makes them insecure.
Anyone can read the podspec or configmap and can grab access to the systems, instead it is recommended to store such sensitive data in ConfigSecret

By default when we store the information in ConfigSecret it will be encrypted rather it would store the information in Base64 Encoded format. We can read the values in plain-text format using the api

A ConfigSecret can be used in 3 ways
1. We can pass these secrets as environment variables to the pod
2. We can mount the secrets as files within the pods
3. The kubelet process itself might use these secrets for connecting to the docker container registry in pulling the images

While storing the sensitive data within the ConfigSecret we can attach type to them to help us identify what type of secret we are storing in ConfigSecret. even thought it is not mandatory to attach type information, it is recommended so that we can understand while accessing the secrets. By default the secret will be stored with default type "apaque"
	
kubernetes has provided built-in secret types, which can be used as a type while defining secret
1. opaque = arbitary data
2. kubernetes.io/service-account-token  = The service account token, is a kubernetes secret or system secret
3. kubernetes.io/dockercfg  = The serialized format of doker config file
4. kubernetes.io/dockerconfigjson = serialized format of docker config json file
5. kubernetes.io/basic-auth       = username/password
6. kubernetes.io/ssh-auth         = ssh keys
7. kubernetes.io/tls              = ssl keys
-------------------------------------------------------------------------------------------
Let us write a ConfigSecret spec in storing an database username and password.
	
roadsterdb-configsecrets.yml
----------------------------
apiVersion: v1
kind: Secret
metadata:
	name: roadsterdbsecrets
type: kubernetes.io/basic-auth
stringData:
	username: root
	password: root
	
	
roadster-pod.yml
-----------------
apiVersion: v1
kind: Pod
metadata:
	name: roadsterpod
	labels:
		app: roadster
spec:
	containers:
		- name: roadster
			image: techsriman/roadster:2.0
			ports:
				- name: http
				  containerPort: 8082
					protocol: TCP
			env:
				- name: "spring.datasource.username"
					valueFrom:
						secretKeyRef:
							name: roadsterdbsecrets
							key: username

				- name: "spring.datasource.password"
					valueFrom:
						secretKeyRef:
							name: roadsterdbsecrets
							key: password
							
Persistent Volume and Persistent Volume Claim
----------------------------------------------
Persistence volumes is a storage that is defined on a kubernetes cluster and the it is created by the kubernetes administrator

Persistent volume claim = is a request for that storage to be consumed by the devops engineer that can be consumed as part of the pod

There are few attributes we can attach to the persistent volume:
1. storageClassName
it talks about the type of storage we wanted to create on the cluster
2. accessMode:
ReadOnly = only pods can read the data from this volume
ReadWriteOnce = only one pod/user can write/read at one time
ReadWriteMultiple = multiple pod/user can read/write at the same time

3. capacity:
size of the storage volume

roadster-pv.yaml
-----------------
apiVersion: v1
kind: PersistentVolume
metadata:
	name: roadsterpv
spec:
	storageClassname: roadsterstorageclass
	capacity:
		storage: 2Gi
	accessMode:
		- ReadWriteMultiple
	hostPath:
		path: /u01/data
		
roadster-pvc.yaml
------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
	name: roadsterpvc
spec:
	storageClassName: roadsterstorageclass
	accessModes:
		- ReadWriteOnce
	resources:
		requests:
			storage: 1Gi
			
roadster-pod.yaml
-----------------
apiVersion: v1
kind: Pod
metadata:
	name: roadsterpod
	labels:
		app: roadster
spec:
	containers:
		- name: roadster
			image: techsriman/roadster:2.0
			ports:
				- name: http
					containerPort: 8082
					protocol: TCP
			volumeMounts:
				- name: roadstervolume
					mountPath: /u01/mysql					
	volumes:
		- name: roadstervolume
			persistentVolumeClaim:
				claimName: roadsterpvc

The kubernetes control plane has 3 components within it
1. api manager
2. scheduler
3. controller

How many types of controllers are there?
There are 5 types of controllers are there
1. ReplicaSet
2. DaemonSet
3. DeploymentSet
4. Job
5. Service

#1. ReplicaSet
Pod is a smallest unit within the kubernetes where multiple containers are kept together and executed within a pod which are having common dependences and lifecyle.
We can create a pod in kubernetes by using pod manfiest/spec. There are few characteristics of a pod are there
	1. A pod manfiest creates a pod on the cluster in running state
	2. Pod will not survive by crash
	
If we want to run #10 pods of the same pod spec, we need to create manually #10 pod specs with different pod name and create the pods using the manifests manually, which is quite difficult task.
In addition we need to monitor and make sure enought number of pods are always running within the cluster, if a pod crashes it will not be brought back automatically we need to monitor and create one more pod in event of crash.
	
So managing multiple replicas of a pod and replacing the pod upon crash is very difficult. To overcome the above problem kubernetes has provided ReplicaSet controller

A ReplicaSet controller can be imagined as a Reconcilation loop, where the ReplicaSet controller loops through all the worknodes across the cluster to observe whether the desired number of replicas specified are met or not, and ensures the cluster meets the desired state.
upon reaching to the desired state of replicas, it monitors the pods on the cluster, and in case of any pod crash, it replaces with a new pod on the cluster to meet the desired state.
	
A ReplicaSet spec always will be written along with Pod Spec embedded inside it as there is no meaning of a ReplicaSet without a Pod since it manages the pod.
	
#5 workernode cluster
#4 replicas of a pod
there is no guarantee that 4 replicas are distributed across different workernodes but kubernetes controlplane makes the best effort in distributing the replicas across the workernodes of the cluster to ensure HA

From the above we can understand we dont have to write a pod spec rather within ReplicaSet spec we define the pod where the ReplicaSet controller bringsup the desired number of pods

ibanking-replicaset.yaml
-------------------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
	name: ibankingreplicaset
	labels:
		app: ibanking
spec:
	replicas: 2
	selector:
		matchLabels:
			app: ibanking
			version: "1.0"
template: [run 3 replicas of the below template]
	metadata:
		labels:
			app: ibanking
			version: "1.0"
	spec:			
		containers:
			- name: ibanking
				image: techsriman/ibanking:1.0
				ports:
					- name: ibankingport
						containerPort: 8082
						protocol: TCP

How to scale a replicaset?
#1 kubectl scale replicaset replicasetname --replicas=2
#2
edit the replicaset by opening the runtime instance of it
kubectl edit replicaset replicasetname
--------------------------------------------------------------------------------------------
2. Deployment
In general deployment is a process of packaging and delivering a software application on the target environment so that it would be available for usage. So in kubernetes world deployment refers to the process of running the containerized application on the kubernetes cluster.
	
There are multiple ways of deploying an containerized application on kubernetes cluster.
	1. pod spec
	2. repliaSet spec
	
There is an another way we can deploy an application on kubernetes cluster which is "deployment".

Instead of writing the pod spec or replicaSet spec in deploying an application we can write deploymentSpec through which we can run an containerized application on the cluster. In the deployment spec we can specify the image and desired number of replicas we wanted to run.
	
The deployment controller takes the deployment spec as an input and ensures the desired number of replicas are running in the cluster by creating replicaSet controller

When there is change in the desired state, the Deployment controller changes from actual state to the new state in a controlled way 

How to work with deployment or how does deployment controller/spec works?
1. create a deployment spec defining the template (image) and desired number of replicas to be running on the cluster.
	per each deployment with replicas specified, the Deployment controller internally creates an ReplicaSet controller to ensure desired state of the cluster
with the above deployment spec the application will be rolled-out on to the cluster

2. if there is a new pod image has been released, we can update the deployment spec with the latest pod template, the deployment controller does the below job in performing the rolling update
A pod template contains several things like
image
container ports
resource requests
liveness and readiness probes
volumes 
etc
when there is a change in any of the section of the pod template, the deployment controller performs a rolling update as below.
	1. It creates an new ReplicaSet controller with the latest pod template with replicas as 25% of the original
	for eg the actual replicas we specified is 4 means, it creates an new replicaSet with replicas as 1, so that with new pod template a new pod will be created on the cluster
	2. upon bringing up the new pod, the deployment controller goes and modifies the previous replicaSet to 3, terminating one of the pod replicas
	eventually it updates the new pod replicaSet one by each terminating 1 at a time on old replicaSet
	in this way the new pod template will be rolledout on to the cluster in controlled way.
		
#3 if the new pod template seems to be not stable, then we can rollback to the previous state of the cluster by issuing a rollback request to the deployment controller
	When we update the pod template, the deployment controller doesnt delete the previsou replicaSet, so that when we issue a rollback, it simply updates the previous replicaSet with desired number of replicas rolling back the pods to the previous state in controlled way
	
#4. we can pause the deployment controller to perform all the updates to the pod template at once and can resume it back

speed-deployment.yaml
---------------------
apiVersion: apps/v1
Kind: Deployment
metadata:
	name: speeddeployment
	labels:
		app: speed
spec:
	replicas: 2
	selector:
		matchLabels:
			app: speed
			version: "1.0"
	template:
		metadata:
			labels:
				app: speed
				version: "1.0"
		spec:
			containers:
				- name: speed
					image: techsriman/speed:1.0
					ports:
						- name: speedport
							containerPort: 8080
							protocol: TCP
							
How to change the pod template within the deployment?
kubectl set image deployment/deploymentName label=image --record
kubectl edit deployment deploymentName

kubectl rollout history deployment deploymentName
kubectl scale deployment deploymentName --replicas=2
								
kubectl rollout undo deployment deploymentName --to-revision=1
								
Deployment
------------
Deployment is another way of deploying containerized application on the kubernetes cluster. Through deployment controller any changes to the pod template can be rolledout in a controlled way.
	
To expose the pod/service/deployment over the cluster we can use port-forwarded on kubernetes.
kubectl port-forward pod/podName sourcePort:targetPort
kubectl port-forward deployment/deployment sourceport:targetport

How to modify the deployment?
1. kubectl set image deployment/deploymentName container=image:tag
2. kubect edit deployment deploymentName


How to see the revision history of rollout changes?
kubectl rollout history deployment/deploymentName
revision    cause
1           initial
2           change color


How to see the rollout status?
kubectl rollout status deployment/deploymentName	

How to rollback or undo a rollout?
kubectl rollout undo deployment/deploymentName

How to go back to the specific revision in the rollout history?
kubectl rollout undo deployment/deploymentName --to-revision=revisionNo

How to scale the deployment?
kubectl scale deployment/airtelcare2deployment --replicas=2	
--------------------------------------------------------------------------------------------
Service

Service is about networking the pods that are running on the kubernetes cluster. By default the pods are accessible within the node on which there are created, but we might wanted a pod to be exposed	
	1. To other pods that are running on the cluster
	2. to be exposed to the external world
	3. to be load balanced the request across the multiple replicas of the pod	
these can be achieved through the help of service
There are 5 types of services are supported by kubernetes cluster
1. ClusteredIp
2. NodePort
3. Loadbalancer
4. Ingress
5. Headless Service

1. ClusterIP
The default service type with which a Service will be created on kubernetes cluster, when we dont specify a type is ClusterIP Service
If we want our pod applications to be load balanced and exposed to the other pod applications with a static ip and fixed port making them accessible within the cluster then we need to use ClusterIP service

usually we want to expose backend application (microservices) ot the frontend applications that are running as pods on the cluster, it can be achieved through ClusterIP Service

The clusterip service will assigns the ip address to the service within the range of cluster only and the service is not accessible to the outside world

#2. NodePort

Service

Service is used for networking the pods that are running on the kubernetes cluster. by default the pods created within the cluster are not accessible to the other pods on the cluster. To make a pod exposed to the other pods or external network we need to use Service

There are 5 types of services are supported by Kubernetes
1. ClusterIP
2. Headless
3. NodePort
4. Load Balancer
5. Ingress

1. ClusterIP
The ClusterIP service is the default service in the kubernetes cluster. if we create a service without specifying a type, by default kubernetes creates it as an cluster ip service.
It is used for making the pods accessing within the cluster. The ClusterIp service is not meant for exposing the pods to the external network

2. NodePort
------------------------------------------------------------------------------------------
ibanking-pod.yaml
------------------
apiVersion: v1
kind: Pod
metadata:
	name: ibanking-pod
	labels:
		app: ibanking
		version: "1.0"
spec
	container
		- name: ibankingcontainer
			image: techsriman/ibanking:2.0
			ports:
				- name: ibankingport
					containerPort: 8080
					protocol: TCP


ibanking-service.yaml
apiVersion: v1
kind: Service
ClusterIP: none
metadata:
	name: ibankingservice
spec:
	selector:
		matchLabels:
			app: ibanking
			version: "1.0"
	port:
		protocol: TCP
		port: 8081
		targetPort: 8080
			

How to create/provision on eks cluster?
1. need to have an aws cloud account.
2. The aws has provided an service called "eks cluster", which is elastic kubernetes service cluster.

The eks is a fully managed service, which provides lot of features/advantages:
1. The AWS Cloud platform eks service itself takes care of provisioning the master/control plane and workernodes and setup the cluster with CNI network and necessary containerization engine.
2. The kubernetes takes care of setting up the control-plane and ensures its HA, the eks guarantees the availability of the control plane always
3. The worker nodes are provisioned and distribtued across the availability zones to ensure the HA of the applications

There are multiple ways we can provision an eks cluster
1. ekscli interface = it is an command-line tool which is provided by aws through which one-shot we can setup kubernetes cluster
2. through aws management console = we can manually configure the cluster through various different options
3. Through automation using terraform or ansible

Let us understand how to setup eks through cloud console
#1. we need vpc, subnets within the aws account
There are 3 ways we can host an eks cluster
	1. public = experimental / development
	2. private & public = typical organization env, where a team manages kubernetes deployments in non-production environments
	3. private = production environment
	
#2. To provision an eks cluster atleast we need 2 subnets under 2 different availability zones of a vpc for HA

#3. we need to create ecr registry and publish docker images into ecr when we choose private or private/public provisioning


#1. setup aws cli on the workstation by following the guide
https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

#2. configure api keys in user account
#3. run aws configure command tosetup aws cli with the account
awscli configure
#4. do a docker login to configure the docker cli work with aws ecr as below
aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/e1b8q8o4

#5. build the covido application and docker image and push to container registry
~/workspace/covido
mvn clean verify
docker image build -t repositoryURI:1.0 .
docker image push repositoryURI:1.0
--------------------------------------------------------------------------------------------
# provision eks cluster
There are 2 parts are there in setting up the eks cluster
1. setup controlplane
	1. vpc = create an vpc with CIDR
	2. subnet
		- 2 public subnets
		- 2 private subnets
		
	3. aws role EKSCluster
	goto IAM policies choose Role
	create a role EKSCluster Role with EKSClusterPolicy inside it
	4. goto Elastic Kubernetes Service and create an cluster
		1. ClusterName
		2. VPC
		3. Subnets
		4. Role
		5. Type (Private & Public)
		6. SecurityGroup
		
5. configure kubeconfig file on workstation by running the below command.
aws eks update-kubeconfig --region region-code --name cluster-name	

aws eks update-kubeconfig --region us-east-2 --name CovidoEKS

2. create worknodes through nodegroup
#1. we need EKSNodeGroupRole
AmazonEKSWorkerNodePolicy
AmazonEC2ContainerRegistryReadOnly
AmazonEKS_CNI_Policy
	


















































































































